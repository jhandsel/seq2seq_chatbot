{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "Original translation code copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Chatbot modifications copyright 2020 Jennifer Handsel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Chatbot with attention\n",
    "\n",
    "I repurposed Tensorflow's seq2seq translation notebook in order to make a chatbot model. \n",
    "\n",
    "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "**Changes**\n",
    "\n",
    "*   Conversation needed a much deeper network than translation. I made the encoder a four layer GRU instead of single layer \n",
    "*   Added attentional feeding, as described in Luong 2015 (Effective Approaches to Attention-based Neural Machine Translation).\n",
    "*   Added hidden-state passing in the decoder. In the original implementation, the output was calculated solely from the previous output and the context vector\n",
    "*   Initialize embedding layer with pre-trained GLOVE word vectors. Experimented with having these weights frozen, but the model's fit was better when these weights were fine-tuned.\n",
    "*   Shared embeddings for input and output\n",
    "*   Implemented a beam search for finding the most probable output sentence, instead of the original greedy approach that gave less satisfactory results\n",
    "*   Added live plots for training, to show loss function and gradient norm (to detect exploding gradients). I experimented with clipping the gradient norm, but I found the training loss was better without this modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tnxXKDjq3jEL",
    "outputId": "c7e22293-b9da-4c71-a56a-ee1400d45f5a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython.display import clear_output\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BdZ_Ilp_QAPx",
    "outputId": "39261d69-61f1-413a-b4af-0cd5f9fade11"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWBnuhZBv2K5"
   },
   "source": [
    "# Ploting functions\n",
    "\n",
    "These functions allow live plotting of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8qTQZXUv5DV"
   },
   "outputs": [],
   "source": [
    "# Plot that updates each time it's fed new data\n",
    "def live_plot(data_dict, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for label,data in data_dict.items():\n",
    "        plt.plot(data, label=label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc='center left') # the plot evolves to the right\n",
    "    plt.show();\n",
    "\n",
    "# fizz = collections.defaultdict(list)\n",
    "# for i in range(10):\n",
    "#     fizz['foo'].append(np.random.random())\n",
    "#     fizz['bar'].append(np.random.random())\n",
    "#     fizz['baz'].append(np.random.random())\n",
    "#     live_plot(fizz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03ciozuUwIls"
   },
   "outputs": [],
   "source": [
    "# Plot that updates each time it's fed new data\n",
    "def live_plot_double(loss_dict, throughput_dict, figsize=(13,5), \n",
    "                                            xlabels=['iteration', 'epoch']):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    plt.figure(figsize=(13,5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    #plt.figure(figsize=(7,5))\n",
    "    for label,data in loss_dict.items():\n",
    "        plt.plot(data, label=label)\n",
    "    #plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(xlabels[0])\n",
    "    plt.legend(loc='center left') # the plot evolves to the right\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    #plt.figure(figsize=(7,5))\n",
    "    for label,data in throughput_dict.items():\n",
    "        plt.plot(data, label=label)\n",
    "    #plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(xlabels[1])\n",
    "    plt.legend(loc='center left') # the plot evolves to the right\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# fizz = collections.defaultdict(list)\n",
    "# buzz = collections.defaultdict(list)\n",
    "# for i in range(10):\n",
    "#     fizz['foo'].append(np.random.random())\n",
    "#     fizz['bar'].append(np.random.random())\n",
    "#     buzz['baz'].append(np.random.random())\n",
    "#     buzz['foobar'].append(np.random.random())\n",
    "#     live_plot_double(fizz, buzz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "# Load dataset\n",
    "\n",
    "The model is trained with Cornell and OpenSubs movie subtitles datasets. We can extract sentence pairs from this dataset:\n",
    "\n",
    "```\n",
    "'Now what did you lose?' 'My son... my mind...'\n",
    "```\n",
    "\n",
    "Here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Clean the sentences by removing special characters.\n",
    "2. Add a *start* and *end* token to each reply.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    # Very needed, as dialogues still contain '-'\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,']+\", \" \", w)\n",
    "    \n",
    "    # Replace multiple spaces with a single\n",
    "    w = re.sub('\\s+', ' ', w)\n",
    "\n",
    "    # Converts to list of tokens as expected for GLOVE vectors\n",
    "    # eg don't -> do n't; the dog's -> the dog 's\n",
    "    w = nltk.word_tokenize(w)\n",
    "\n",
    "    w = ' '.join(w)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Clean the sentences\n",
    "# 2. Return sentence two lists of sentences A and B\n",
    "# max_len is maximum number of words to extract\n",
    "def load_sentences(data_path, max_len=25, max_sentences=30000):\n",
    "    print('Loading sentences')\n",
    "    data = pd.read_csv(data_path, header=None, sep='\\t', dtype=str, \n",
    "                                        na_filter=False).values.tolist()\n",
    "\n",
    "    # Clean sentences\n",
    "    # Bit of a hack here to speed things up - may miss some sentences\n",
    "    # Make sure dataset is pre-shuffled!\n",
    "    sentences = [(preprocess_sentence(x[0]), preprocess_sentence(x[1])) for x in tqdm(data[:max_sentences*2])]\n",
    "\n",
    "    # Drop long sentences\n",
    "    sentences = [(x[0], x[1]) for x in sentences if \n",
    "     ( (len(x[0].split(' ')) <= max_len) and (len(x[1].split(' ')) <= max_len) )]\n",
    "\n",
    "    # Extract as separate lists and pad with <start> and <end>\n",
    "    sentA = [x[0] for x in sentences[0:max_sentences]]\n",
    "    sentB = ['<start> ' + x[1] + ' <end>' for x in sentences[0:max_sentences]]\n",
    "\n",
    "    print('Got %d sentences pairs' % (len(sentA)))\n",
    "\n",
    "    return sentA, sentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmMZQpdO60dt"
   },
   "outputs": [],
   "source": [
    "# Create a tokenizer that converts sentence into list of ints\n",
    "# Return tokenized inputs and outputs\n",
    "# Takes two lists, of input and output sentences\n",
    "def tokenize(input_list, target_list, max_words=25000):\n",
    "    \n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',\n",
    "                                            num_words=max_words, oov_token='<unk>')\n",
    "    lang_tokenizer.fit_on_texts(input_list + target_list)\n",
    "\n",
    "    # Convert sentences into list of integers\n",
    "    input_tensor = lang_tokenizer.texts_to_sequences(input_list)\n",
    "    # Pad to uniform length with zeros\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, padding='post')\n",
    "    \n",
    "    target_tensor = lang_tokenizer.texts_to_sequences(target_list)\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post')\n",
    "\n",
    "    return input_tensor, target_tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "# Load sparse encoding vectors and 'tokenizer' for generating encoding vectors\n",
    "def load_dataset(dataset_path, max_len=30, max_sent=30000, max_words=25000):\n",
    "\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "    # creating cleaned input, output pairs\n",
    "    sentA, sentB = load_sentences(dataset_path, max_len=max_len, max_sentences=max_sent)\n",
    "\n",
    "    input_tensor, target_tensor, lang_tokenizer = tokenize(sentA, sentB, max_words=max_words)\n",
    "\n",
    "    # Print vital statistics\n",
    "    max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "    print('Max length of input: %d' % max_length_inp)\n",
    "    print('Max length of target (including <start> and <end>): %d' % max_length_targ)\n",
    "\n",
    "    print('Number of items in vocab: %d' % len(lang_tokenizer.word_index))\n",
    "\n",
    "    return input_tensor, target_tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "cnxC7q-j3jFD",
    "outputId": "60d2657f-70ae-4fc1-e22a-a2285a9a0cba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data should be tab-separated, with column one containing a sentence,\n",
    "# and column two containing the response\n",
    "dataset_path = '/path/to/dataset.tsv'\n",
    "\n",
    "max_len = 30\n",
    "max_sent = 3000000\n",
    "max_words = 50000\n",
    "input_tensor, target_tensor, lang = load_dataset(dataset_path, max_len, max_sent, max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zgb8fn8rp7fv"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xfXXyIwqAsp1",
    "outputId": "3866fe73-e091-4787-88ae-b10c8e8b7b2a"
   },
   "outputs": [],
   "source": [
    "example_tensor = lang.texts_to_sequences(['<start> how are you gentlemen fufufu <end>'])\n",
    "print(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z4dqnWHwAxZL",
    "outputId": "d610e285-978d-4662-bf31-40177e4824e8"
   },
   "outputs": [],
   "source": [
    "print([lang.index_word[x] for x in example_tensor[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Q-fWDxUp1Yr"
   },
   "source": [
    "# Create training set and validation set\n",
    "\n",
    "Here we split the input conversations 80/20 into a training set and a validation set. The sets are shuffled to avoid bias within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4QILQkOs3jFG",
    "outputId": "e49668a9-f162-4803-c37e-b6d114b54892"
   },
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
    "    train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYgYI0ULqib8"
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "VXukARTDd7MT",
    "outputId": "98df6c6a-bb8f-4ee0-8a0c-3b04bd6c7887"
   },
   "outputs": [],
   "source": [
    "print (\"Input Sentence; index to word mapping\")\n",
    "convert(lang, input_tensor_train[25])\n",
    "print ()\n",
    "print (\"Target Sentence; index to word mapping\")\n",
    "convert(lang, target_tensor_train[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QyUIhL_VOQzZ",
    "outputId": "3d137ccd-236b-47da-a9db-c41a7a976ada"
   },
   "outputs": [],
   "source": [
    "print('Number of words found: %d' % len(lang.index_word))\n",
    "print('Limiting number of words to: %d' % lang.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "## Create a tf.data training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "# Number of items in training set\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "# Batching is necessary to prevent GPU running out of memory - so use in both training and validation\n",
    "# Can evaluate batch size by measuring training sample throughput per unit time\n",
    "BATCH_SIZE = 512\n",
    "# Number of samples per batch\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "#vocab_size = len(lang.word_index)+1\n",
    "vocab_size = lang.num_words+1\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "train_set = train_set.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGyTbDJsq8Nc"
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZmQ27sgQAQ0",
    "outputId": "1909336c-c881-4aff-9790-02cfbbd5b4a8"
   },
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qc6-NK1GtWQt",
    "outputId": "9ea391aa-1c8b-4a4e-d948-996ee8735d5c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_set))\n",
    "example_input_batch.shape, example_target_batch.shape\n",
    "#print(example_input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lJbJOtKQAQ6"
   },
   "source": [
    "## Create a tf.data validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMF9FzuMQAQ7"
   },
   "outputs": [],
   "source": [
    "# VAL_BUFFER SIZE is number of items in validation set\n",
    "VAL_BUFFER_SIZE = len(input_tensor_val)\n",
    "# Number of samples per batch\n",
    "val_steps_per_epoch = VAL_BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "val_set = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(VAL_BUFFER_SIZE)\n",
    "val_set = val_set.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0R1bDYrrAP_"
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xC3ymx_RQAQ9",
    "outputId": "a037a3aa-6111-4695-9e83-271aec15520a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_val_input_batch, example_val_target_batch = next(iter(val_set))\n",
    "example_val_input_batch.shape, example_val_target_batch.shape\n",
    "#print(example_val_input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbOl4aUNLmu0"
   },
   "source": [
    "# Load Word Vectors\n",
    "\n",
    "To improve model performance, the embedding layer will used pretrained GLOVE vectors for its weights.\n",
    "\n",
    "Best practice is to tokenize the texts in the same way that the glove vectors were tokenized - lower case + nltk tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tFcYuIrkMKkk",
    "outputId": "52426d29-d09c-480a-cd3f-4c7d39dfe8ef"
   },
   "outputs": [],
   "source": [
    "GLOVE_FILE = 'glove.6B.300d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_dim = len(embeddings_dict['the'])\n",
    "print('Found %s word vectors, length %d.' % (len(embeddings_dict), embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2JIU2-Jibna"
   },
   "outputs": [],
   "source": [
    "# Create embedding matrix from words in vocab\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in lang.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_dict.get(word)\n",
    "        # words not found in embedding index will be initialized randomly\n",
    "        if embedding_vector is None:\n",
    "            embedding_matrix[i] = np.asarray(np.random.random_sample(embedding_dim)-0.5, dtype='float32')\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lIUnMPBTej8"
   },
   "source": [
    "# Initialize Shared Word Embeddings\n",
    "\n",
    "The embedding layer is shared between the encoder and decoder. This is more efficient and results in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZqrjyzdZTmys"
   },
   "outputs": [],
   "source": [
    "shared_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a24411NSQARA"
   },
   "source": [
    "# Define Encoder\n",
    "\n",
    "Simple 4-layer GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout_prob):\n",
    "        # Initialize the base class\n",
    "        super().__init__()\n",
    "        # Initialize variables specific to this class\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='orthogonal',\n",
    "                                   dropout=dropout_prob,\n",
    "                                   go_backwards=True)\n",
    "        self.gru2 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='orthogonal',\n",
    "                                   dropout=dropout_prob,\n",
    "                                   go_backwards=True)\n",
    "        self.gru3 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='orthogonal',\n",
    "                                   dropout=dropout_prob,\n",
    "                                   go_backwards=True)\n",
    "        self.gru4 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='orthogonal',\n",
    "                                   dropout=dropout_prob,\n",
    "                                   go_backwards=True)\n",
    "        \n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        #x = self.embedding(x)\n",
    "        x_emb = shared_embedding(x)\n",
    "        # We get matrix of outputs from each step, as well as separate final state\n",
    "        output, state1 = self.gru1(x_emb, training=training)\n",
    "        output, state2 = self.gru2(output, training=training)\n",
    "        output, state3 = self.gru3(output, training=training)\n",
    "        output, state4 = self.gru4(output, training=training)\n",
    "        state = tf.math.accumulate_n([state1, state2, state3, state4])\n",
    "        #state = tf.concat([state1, state2, state3, state4], axis=-1)\n",
    "        # output = tf.concat([output1, output2], axis=-1)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    # For random initialization during training\n",
    "    def initialize_hidden_state_random(self, stddev=0.3):\n",
    "        tf.random.normal((self.batch_sz, self.enc_units), stddev=stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBM_0F_GQARF"
   },
   "source": [
    "# Define Attention\n",
    "\n",
    "Bahdanau-style additive attention using a trained tanh layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # Dense layer with no activation: just think of it as a matrix multiplication\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # And this one is vector multiplication, as the output dimension is 1\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Flr-pj0fQARJ"
   },
   "source": [
    "# Define Decoder\n",
    "\n",
    "Calculates context based on current hidden state and encoder output, and predicts following word with a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "\n",
    "        #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='orthogonal',\n",
    "                                       kernel_initializer='orthogonal',\n",
    "                                       dropout=dropout_prob,\n",
    "                                       recurrent_dropout=dropout_prob)\n",
    "\n",
    "        # Dense layer for outputing word logits\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "            \n",
    "        # Include attention layer\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # hidden is previous hidden state\n",
    "    # x is tokenized batch of input words for current position, shape == (batch_size, 1)\n",
    "    # dec_state shape == (batch_size, hidden_size)\n",
    "    # context shape == (batch_size, hidden_size)\n",
    "    # enc_output is tensor containing encoder outputs, shape == (batch_size, max_length, hidden_size)\n",
    "    #def call(self, x, hidden, enc_output):\n",
    "    def call(self, x, dec_state, context, enc_output, training=True):\n",
    "        # Encoder outputs and previous hidden state used to calculate context vector\n",
    "        # enc_output \n",
    "        # context_vector shape == (batch_size, hidden_size)\n",
    "        #context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        ################################\n",
    "        #  Calculate new decoder state GRU #\n",
    "        ################################\n",
    "\n",
    "        # Embed input batch of words\n",
    "        # Shape == (batch_size, 1, embedding_dim)\n",
    "        x_emb = shared_embedding(x)\n",
    "\n",
    "        # Combine context and x; calculate new state\n",
    "        # Need to convert context vector to shape == (batch_size, 1, hidden_size)\n",
    "        context_x = tf.concat([tf.expand_dims(context, 1), x_emb], axis=-1)\n",
    "        _, dec_state = self.gru(context_x, initial_state=dec_state, training=training)\n",
    "\n",
    "        # Reshape output\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        #gru_output = tf.reshape(gru_output, (-1, gru_output.shape[2]))\n",
    "\n",
    "        ####################\n",
    "        # Calculate logits #\n",
    "        ####################\n",
    "\n",
    "        # Calculate new context from new state\n",
    "        context, attention_weights = self.attention(dec_state, enc_output)\n",
    "\n",
    "        # Combine context and new state\n",
    "        context_state = tf.concat([context, dec_state], axis=1)\n",
    "        # Calculate logits for next word y\n",
    "        # logits shape == (batch_size, vocab)\n",
    "        logits = self.fc(context_state)\n",
    "\n",
    "        return logits, dec_state, context, attention_weights\n",
    "\n",
    "    # Use this to get a zero initial decoder state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdK9ObXaQARC"
   },
   "source": [
    "# Initialize encoder, decoder and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropout will be applied to non-recurrent weights of each GRU\n",
    "# (both encoder and decoder)\n",
    "DROPOUT_PROBABILITY = 0.2\n",
    "\n",
    "# Size of hidden state\n",
    "units = 256\n",
    "\n",
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE, DROPOUT_PROBABILITY)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE, DROPOUT_PROBABILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWtjuF78uoMb"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# Accepts batch of word encodings\n",
    "# Real is an array of shape == batch_size\n",
    "# Pred has shape == (batch_size x vocab_size)\n",
    "def loss_function(real, pred):\n",
    "    # Ignore end padding positions\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # loss_ has shape == batch_size\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LlvwWdxujEe"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 shared_embedding=shared_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsBHjGBMsZNs"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "igtg2FB5M1od",
    "outputId": "2edcd645-afb0-4630-cf27-5e1e5e3aeaf3"
   },
   "outputs": [],
   "source": [
    "example_input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "V_G0UVTQscB0",
    "outputId": "78ad8e6c-268b-48ee-fa7f-3898cae67960"
   },
   "outputs": [],
   "source": [
    "# sample input\n",
    "sample_output, sample_hidden = encoder(example_input_batch, training=False)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "sample_decoder_output, sample_decoder_state, sample_context, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), \n",
    "                        sample_hidden, tf.random.uniform((BATCH_SIZE, units)), sample_output, training=False)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "print ('Decoder state shape: (batch_size, units) {}'.format(sample_decoder_state.shape))\n",
    "print ('Context shape: (batch_size, units) {}'.format(sample_context.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "# Training / Validation Steps\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "# @ sign is a decorator\n",
    "# Basically it means, pass the function below as an argument to tf.function\n",
    "# tf.function compiles input into a callable tensorflow graph - \n",
    "# it instantiates a separate graph for every unique set of input shapes and datatypes.\n",
    "# Input should be tensorflow datatypes\n",
    "#\n",
    "# In short, it turns function below into a tensorflow function, speeding code up\n",
    "# immensely\n",
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "\n",
    "    # Record output of operations on trainable variables for easy differentiation\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        # Get encoder output for whole input sentence\n",
    "        enc_output, enc_hidden = encoder(inp, training=True)\n",
    "\n",
    "        # Using start token as initial decoder input\n",
    "        dec_x = tf.expand_dims([lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Get initial decoder state - copy from encoder\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # Initial context should be blank state, same shape as encoder hidden state\n",
    "        context = encoder.initialize_hidden_state()\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # Calculates attention using previous hidden state and encoder output\n",
    "            logits, dec_hidden, context, _ = decoder(dec_x, dec_hidden, context, enc_output, training=True)\n",
    "\n",
    "            # Compute scalar loss for current position in sentence\n",
    "            loss += loss_function(targ[:, t], logits)\n",
    "\n",
    "            # Compute perplexity - using average sentence perplexity\n",
    "            #log_perplexity = tf.add(log_perplexity, loss_vec)\n",
    "            #print(log_perplexity)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_x = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    # Get average sentence perplexity\n",
    "    #perplexity_vec = tf.exp(log_perplexity)\n",
    "    #batch_perplexity = tf.reduce_mean(log_perplexity)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # Clip by global norm\n",
    "    #gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "    # Clip individual norms\n",
    "    # gradients = [\n",
    "    #     None if gradient is None else tf.clip_by_norm(gradient, 1.0)\n",
    "    #     for gradient in gradients]\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Get gradient norms\n",
    "    # Note: getting extra norms this way is very expensive, not recommended\n",
    "    # for production runs, just to diagnose problems\n",
    "    # grads_encoder = tape.gradient(loss, encoder.trainable_variables)\n",
    "    # grads_decoder = tape.gradient(loss, decoder.trainable_variables)\n",
    "    # del tape\n",
    "    # gnorm_encoder = tf.linalg.global_norm(grads_encoder)\n",
    "    # gnorm_decoder = tf.linalg.global_norm(grads_decoder)\n",
    "\n",
    "    gnorm = tf.linalg.global_norm(gradients)\n",
    "\n",
    "    return batch_loss, gnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XV0O2LsyQARc"
   },
   "outputs": [],
   "source": [
    "# Instantiate accuracy calculator\n",
    "acc_object = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def validation_step(inp, targ):\n",
    "    \n",
    "    # Initialize running total loss\n",
    "    loss = 0\n",
    "    #log_perplexity = tf.zeros((BATCH_SIZE))\n",
    "\n",
    "    # Reset running total accuracy\n",
    "    acc_object.reset_states()\n",
    "    \n",
    "    # Send input to encoder\n",
    "    enc_output, enc_hidden = encoder(inp, training=False)\n",
    "\n",
    "    # Using start token as initial decoder input\n",
    "    dec_x = tf.expand_dims([lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Get initial decoder state - copy from encoder\n",
    "    dec_hidden = enc_hidden\n",
    "    # Initialize as zero tensor\n",
    "    #dec_hidden = decoder.initialize_hidden_state()\n",
    "\n",
    "    # Get initial context\n",
    "    # This is wrong\n",
    "    #context, _ = decoder.attention(dec_hidden, enc_output)\n",
    "\n",
    "    # Initial context should be blank state, same shape as encoder hidden state\n",
    "    context = encoder.initialize_hidden_state()\n",
    "\n",
    "    # targ has dimensions BATCH_SIZE * SENT_LENGTH\n",
    "    for t in range(1, targ.shape[1]):\n",
    "        # predictions shape == (batch_size, vocab)\n",
    "        logits, dec_hidden, context, _ = decoder(dec_x, dec_hidden, context, enc_output, training=False)\n",
    "\n",
    "        # Compute scalar loss for current position in sentence\n",
    "        #loss += tf.reduce_mean(loss_function(targ[:, t], logits))\n",
    "\n",
    "        # Compute scalar loss for current position in sentence\n",
    "        loss += loss_function(targ[:, t], logits)\n",
    "\n",
    "        # Compute perplexity - using average sentence perplexity\n",
    "        #log_perplexity = tf.add(log_perplexity, loss_vec)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        mask = tf.math.logical_not(tf.math.equal(targ[:, t], 0))\n",
    "        acc_object.update_state(targ[:, t], logits, sample_weight=mask)\n",
    "\n",
    "        # predicted_ids is a vector of length batch_size\n",
    "        #predicted_ids = tf.argmax(logits, 1)\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        #dec_x = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_x = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # Normalize the loss\n",
    "    # Essentially, this is weighted average cross-entropy\n",
    "    # (log-perplexity) per symbol.\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    # Get average sentence perplexity\n",
    "    #perplexity_vec = tf.exp(log_perplexity)\n",
    "    #print(log_perplexity.numpy()[5])\n",
    "    #print(np.exp(log_perplexity.numpy()[5]))\n",
    "    #batch_perplexity = tf.reduce_mean(log_perplexity)\n",
    "\n",
    "    # Accuracy is already normalized\n",
    "    #batch_accuracy = acc_object.result().numpy()\n",
    "    batch_accuracy = acc_object.result()\n",
    "\n",
    "    return batch_loss, batch_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5SE5AXP7JC_"
   },
   "source": [
    "# Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "1o6MEKGlgLuZ",
    "outputId": "4c1c17a7-bec2-4b1e-cc80-f9fd88a36685"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "# Dictionary to contain loss, accuracy, etc.\n",
    "epoch_history = collections.defaultdict(list)\n",
    "# Store train/validation loss at each iteration\n",
    "batch_history = collections.defaultdict(list)\n",
    "throughput_history = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train model on training set\n",
    "    train_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_set.take(steps_per_epoch)):\n",
    "        batch_loss, gnorm = train_step(inp, targ)\n",
    "        train_loss += batch_loss\n",
    "        batch_history['training_loss'].append(batch_loss)\n",
    "        batch_history['gradient_norm'].append(gnorm)\n",
    "        # batch_history['gradient_norm_encoder'].append(gnorm_encoder)\n",
    "        # batch_history['gradient_norm_decoder'].append(gnorm_decoder)\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    \n",
    "    # Get timing statistics\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    throughput = BUFFER_SIZE/duration\n",
    "    throughput_history['sample_per_sec'].append(throughput)\n",
    "    \n",
    "    # Record training loss\n",
    "    epoch_loss = train_loss / steps_per_epoch\n",
    "    epoch_history['training_loss'].append(epoch_loss)\n",
    "            \n",
    "    # Calculate validation loss\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    for (batch, (inp, targ)) in enumerate(val_set.take(val_steps_per_epoch)):\n",
    "        vbatch_loss, vbatch_accuracy = validation_step(inp, targ)\n",
    "        val_loss += vbatch_loss\n",
    "        val_accuracy += vbatch_accuracy\n",
    "\n",
    "    # Record validation loss\n",
    "    val_epoch_loss = val_loss / val_steps_per_epoch\n",
    "    epoch_history['validation_loss'].append(val_epoch_loss)\n",
    "    val_epoch_accuracy = val_accuracy / val_steps_per_epoch\n",
    "    epoch_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "\n",
    "    # Plot loss\n",
    "    #live_plot(loss_history)\n",
    "    #live_plot_double(epoch_history, throughput_history)\n",
    "    live_plot_double(batch_history, epoch_history)\n",
    "    \n",
    "    # Print results and plot loss\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, epoch_loss))\n",
    "    print('Validation {} Loss {:.4f}'.format(epoch + 1, val_epoch_loss))\n",
    "    print('Validation {} Accuracy {:.4f}'.format(epoch + 1, val_epoch_accuracy))\n",
    "    print('')\n",
    "    # Get epoch with lowest validation loss\n",
    "    best_epoch = np.argmin(epoch_history['validation_loss'])\n",
    "    print('Minimum Validation Loss ({}) {:.4f}'.format(best_epoch+1, \n",
    "                            epoch_history['validation_loss'][best_epoch]))\n",
    "    print('Corresponding Validation Accuracy {:.4f}'.format(\n",
    "                            epoch_history['validation_accuracy'][best_epoch]))\n",
    "    print('')\n",
    "    print('Time taken to train for 1 epoch {:.2f} sec'.format(duration))\n",
    "    print('Processed {:.2f} samples / sec\\n'.format(throughput))\n",
    "\n",
    "    # Save checkpoint if validation loss has improved\n",
    "    if len(epoch_history['validation_loss']) > 1:\n",
    "        if epoch_history['validation_loss'][-1] < epoch_history['validation_loss'][-2]:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    # Stop if validation loss keeps going up\n",
    "    if len(epoch_history['validation_loss']) > 3:\n",
    "        if epoch_history['validation_loss'][-1] > \\\n",
    "                                    epoch_history['validation_loss'][-4]:\n",
    "            print('Validation loss has increased for three epochs, stopping')\n",
    "            break\n",
    "\n",
    "final_time = time.time()\n",
    "total_time = final_time - initial_time\n",
    "print('Training took %.2f hours' % (total_time / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "# Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5zKa73GW_oP2",
    "outputId": "5c02009e-9fee-458f-f399-5a0ebe59888c"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_dir = './checkpoints/'\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "# Evaluate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Randomly picks from the top two predictions at each step\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length_targ=30, max_length_inp=30):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = []\n",
    "    for w in sentence.split(' '):\n",
    "        try:\n",
    "            if lang.word_index[w] < (vocab_size):\n",
    "                inputs.append(lang.word_index[w])\n",
    "            else:\n",
    "                inputs.append(lang.word_index['<unk>'])\n",
    "        except:\n",
    "            inputs.append(lang.word_index['<unk>'])\n",
    "\n",
    "    #inputs = [lang.word_index[i] for i in sentence.split(' ')]\n",
    "\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_output, enc_hidden = encoder(inputs, training=False)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([lang.word_index['<start>']], 0)\n",
    "\n",
    "    # Get initial context\n",
    "    #context = encoder.initialize_hidden_state()\n",
    "    context = tf.zeros((1, units))\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, context, attention_weights = decoder(dec_input,\n",
    "                                dec_hidden, context, enc_output, training=False)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        # Pick top predicted next words\n",
    "        #predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        _, predicted_ids = tf.math.top_k(predictions[0], k=2)\n",
    "        predicted_ids = predicted_ids.numpy()\n",
    "\n",
    "        # Remove <unk>\n",
    "        predicted_ids = [i for i in predicted_ids if lang.index_word[i] != '<unk>']\n",
    "\n",
    "        # Stop if most likely tag is end tag\n",
    "        if lang.index_word[predicted_ids[0]] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # Otherwise pick one of the top words\n",
    "        predicted_id = random.choice(predicted_ids)\n",
    "\n",
    "        if lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        result += lang.index_word[predicted_id] + ' '\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def respond(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuruFaaHA9um"
   },
   "source": [
    "# Beam Search\n",
    "This search tries to find the most probable sentence with a breadth-first search. It evaluates several predicted sentences (controlled by beam_width), keeping only the most probable sequences.\n",
    "\n",
    "It gives better replies than a totally gready search (beam_width=1), and is far more coherent than choosing a random word from the top k predictions at each stage.\n",
    "\n",
    "I find a narrow beam width (5-20) gives quite vulgar replies, a larger widgh (100-200) is more refined.\n",
    "\n",
    "The final response is chosen randomly, weighted by probability. This can be relaxed somewhat to allow less probable replies with fuzzy_choice=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMTgu7V7BCRT"
   },
   "outputs": [],
   "source": [
    "def evaluate_beam(sentence, max_length_inp=30, max_length_targ=None, \n",
    "                        beam_width=10, fuzzy_choice=False):\n",
    "\n",
    "    # Final results\n",
    "    final_results = []\n",
    "    # Cumulative logits for each result\n",
    "    final_logits = []\n",
    "\n",
    "    # Clean sentence\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    if max_length_targ == None:\n",
    "        max_length_targ = 3*len(sentence.split(' '))\n",
    "\n",
    "    # Tokenize inputs and pad\n",
    "    inputs = []\n",
    "    for w in sentence.split(' '):\n",
    "        try:\n",
    "            if lang.word_index[w] < (vocab_size):\n",
    "                inputs.append(lang.word_index[w])\n",
    "            else:\n",
    "                inputs.append(lang.word_index['<unk>'])\n",
    "        except:\n",
    "            inputs.append(lang.word_index['<unk>'])\n",
    "\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    #result = ['', '', '']\n",
    "    result = ['' for x in range(beam_width)]\n",
    "\n",
    "    # Get encoder output\n",
    "    enc_output, enc_hidden = encoder(inputs, training=False)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([lang.word_index['<start>']], 0)\n",
    "\n",
    "    #print('First dec_input shape: {}'.format(dec_input.shape))\n",
    "\n",
    "    # Get initial context\n",
    "    #context = encoder.initialize_hidden_state()\n",
    "    context = tf.zeros((1, units))\n",
    "\n",
    "    # Get initial decoder output\n",
    "    # dec_input has shape == (batch_size, 1)\n",
    "    # dec_hidden shape == (batch_size, hidden_size)\n",
    "    # context shape == (batch_size, hidden_size)\n",
    "    # enc_output is tensor containing encoder outputs, shape == (batch_size, max_length, hidden_size)\n",
    "    logits, dec_hidden, context, _ = decoder(dec_input,\n",
    "                            dec_hidden, context, enc_output, training=False)\n",
    "\n",
    "    # Softmax logits\n",
    "    logits = tf.nn.log_softmax(logits)\n",
    "\n",
    "    # Get top three predictions - beam search with beam_width\n",
    "    top_logits, predicted_ids = tf.math.top_k(logits[0], k=beam_width)\n",
    "\n",
    "    # Add word to results\n",
    "    for i, idx in enumerate(predicted_ids.numpy()):\n",
    "        result[i] += lang.index_word[idx] + ' '\n",
    "\n",
    "    #print(\"First step top words: {}\".format(result))\n",
    "\n",
    "    # Stack hidden state\n",
    "    dec_hidden_list = [dec_hidden for x in range(beam_width)]\n",
    "    dec_hidden_b = tf.concat(dec_hidden_list, axis=0)\n",
    "    #print(\"Batch hidden state shape: {}\".format(dec_hidden_b.shape))\n",
    "\n",
    "    # Stack context\n",
    "    context_list = [context for x in range(beam_width)]\n",
    "    context_b = tf.concat(context_list, axis=0)\n",
    "    #print(\"Batch context shape: {}\".format(context_b.shape))\n",
    "\n",
    "    # Stack encoder state\n",
    "    enc_output_list = [enc_output for x in range(beam_width)]\n",
    "    enc_output_b = tf.concat(enc_output_list, axis=0)\n",
    "    #print(\"enc_output shape: {}\".format(enc_output.shape))\n",
    "    #print(\"Batch enc_output shape: {}\".format(enc_output_b.shape))\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        # Run next encoder step with batch of top results\n",
    "        # Resize to shape == (beam_width, 1)\n",
    "        predicted_ids = tf.expand_dims(predicted_ids, 1)\n",
    "        #print(\"predicted_ids shape: {}\".format(predicted_ids.shape))\n",
    "        top_logits = tf.expand_dims(top_logits, 1)\n",
    "        #print(top_logits.numpy())\n",
    "\n",
    "        # Run through network again\n",
    "        # predictions shape == (beam_width, vocab)\n",
    "        logits_batch, dec_hidden_b, context_b, _ = decoder(predicted_ids,\n",
    "                                dec_hidden_b, context_b, enc_output_b, training=False)\n",
    "        \n",
    "        # Softmax logits\n",
    "        logits_batch = tf.nn.log_softmax(logits_batch)\n",
    "\n",
    "        #print('Logits batch shape: {}'.format(logits_batch.shape))\n",
    "        #print('Hidden state shape: {}'.format(dec_hidden_b.shape))\n",
    "        \n",
    "        # Basically: need to add top logits to predictions_b\n",
    "        # Tensorflow has automagic singleton expansion!\n",
    "        logits_conditional = top_logits + logits_batch\n",
    "\n",
    "        # In fact, since we have to mess about, let's flatten first\n",
    "        # logits_conditional now shape == (vocab * beam_width)\n",
    "        logits_conditional = tf.reshape(logits_conditional, [-1])\n",
    "\n",
    "        #print(\"Flattened logits shape: {}\".format(logits_conditional.shape))\n",
    "\n",
    "        # Now get top 3\n",
    "        # Cumulative log probability\n",
    "        # Index of most probable word to come next\n",
    "        top_logits, indices = tf.math.top_k(logits_conditional, k=beam_width)\n",
    "\n",
    "        # Get corresponding path for each in the top 3 (i.e. 0, 1, 2)\n",
    "        path = indices.numpy() // vocab_size\n",
    "\n",
    "        #print(\"Flattened top indices: {}\".format(indices))\n",
    "        #print(\"Corresponding paths: {}\".format(path))\n",
    "\n",
    "        # Pick results to carry over\n",
    "        #print(\"\")\n",
    "        #print(result)\n",
    "        #print(path)\n",
    "        #print(\"\")\n",
    "        result = [result[p] for p in path]\n",
    "\n",
    "        #print('Carry over result: {}'.format(result))\n",
    "\n",
    "        predicted_ids = []\n",
    "        # Get get new predicted ids\n",
    "        for p, idx in zip(path, indices.numpy()):\n",
    "            #print(idx - p*vocab_size)\n",
    "            predicted_ids += [idx - p*vocab_size]\n",
    "\n",
    "        #print(predicted_ids)\n",
    "        \n",
    "        # Add word to results\n",
    "        ended = []\n",
    "        for i, idx in enumerate(predicted_ids):\n",
    "            next_word = lang.index_word[idx]\n",
    "            if next_word == '<end>':\n",
    "                final_results.append(result[i].rstrip())\n",
    "                final_logits.append(top_logits.numpy()[i])\n",
    "                ended.append(i)\n",
    "                beam_width -= 1\n",
    "            else:\n",
    "                result[i] += next_word + ' '\n",
    "\n",
    "        # Remove ended sentences from pool\n",
    "        result = [result[i] for i in range(len(result)) if i not in ended]\n",
    "        # Break if have no results left\n",
    "        if (len(result) == 0) or (beam_width == 0):\n",
    "            break\n",
    "        # Do same for path and predicted ids\n",
    "        path = [path[i] for i in range(len(path)) if i not in ended]\n",
    "        predicted_ids = [predicted_ids[i] for i in range(len(predicted_ids)) if i not in ended]\n",
    "        #print(top_logits.shape)\n",
    "\n",
    "        \n",
    "        top_logits = [top_logits.numpy()[i] for i in range(len(top_logits)) if i not in ended]\n",
    "        top_logits = tf.stack(top_logits)\n",
    "\n",
    "        # # Regenerate correct batchsize of encoder output\n",
    "        enc_output_list = [enc_output for x in range(len(result))]\n",
    "        enc_output_b = tf.concat(enc_output_list, axis=0)\n",
    "\n",
    "        # print(\"Final results: {}\".format(final_results))\n",
    "        # print(\"Top sequences: {}\".format(result))\n",
    "        # print(\"Path: {}\".format(path))\n",
    "        # print(top_logits.shape)\n",
    "        # print(\"\")\n",
    "\n",
    "        # Get hidden states to carry over, shape == (3, units)\n",
    "        hidden_list = [dec_hidden_b[x] for x in path]\n",
    "        #print(dec_hidden_b.shape)\n",
    "        dec_hidden_b = tf.stack(hidden_list)\n",
    "        #print(dec_hidden_b.shape)\n",
    "\n",
    "        # Get contexts to carry over\n",
    "        context_list = [context_b[x] for x in path]\n",
    "        #print(context_b.shape)\n",
    "        context_b = tf.stack(context_list)\n",
    "        #print(context_b.shape)\n",
    "\n",
    "    if len(final_results) == 0:\n",
    "        return np.random.choice([\"I don't understand.\", \"what do you mean?\"])\n",
    "\n",
    "    # Fuzzy choice will get more erratic (and entertaining) results\n",
    "    if fuzzy_choice == True:\n",
    "        l_sum = sum(final_logits)\n",
    "        p = [x/l_sum for x in final_logits]\n",
    "    else:\n",
    "        p = np.exp(final_logits) / sum(np.exp(final_logits))\n",
    "\n",
    "    sentence = np.random.choice(final_results, p=p)\n",
    "\n",
    "    if '<unk>' in sentence.split(' '):\n",
    "        sentence = np.random.choice(['all your base are belong to us', \n",
    "                                     \"you are on your way to destruction\", \n",
    "                                     \"someone set up us the bomb\", \n",
    "                                     \"what you say?\"])\n",
    "    else:\n",
    "        sentence = sentence.replace(\" i \", \" I \")\n",
    "        sentence = re.sub(\"^i \", \"I \", sentence)\n",
    "        sentence = sentence.replace(\" n't\", \"n't\")\n",
    "        sentence = sentence.replace(\" '\", \"'\")\n",
    "        sentence = sentence.replace(\"gon na\", \"gonna\")\n",
    "        sentence = sentence.replace(\"wan na\", \"wanna\")\n",
    "        sentence = sentence.replace(\" .\", \".\")\n",
    "        sentence = sentence.replace(\" !\", \"!\")\n",
    "        sentence = sentence.replace(\" ?\", \"?\")\n",
    "        sentence = sentence.replace(\" ,\", \",\")\n",
    "\n",
    "    return sentence"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VWBnuhZBv2K5",
    "9XFIXNXJnwTB",
    "FcWU4f6Kgf9C",
    "mU3Ce8M6I3rz"
   ],
   "name": "seq2seq_multigru_wordvecs",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
